{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62G8M_viUJp"
      },
      "source": [
        "# Технологии машинного обучения. Лабораторная работа №7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13l6BbxKhCKp",
        "outputId": "baab6307-3b1c-4048-b529-cf8c999a4530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.10/dist-packages (2.0.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gym[box2d]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "WBeQhPi2S4m5"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y python-opengl > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT4N3qYviUJr",
        "outputId": "662d688f-7191-4cd3-9901-7a998c31d1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOUCe2D0iUJu"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "aXKbbMC-kmuv"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_actions: int,\n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    self.actor = layers.Dense(num_actions)\n",
        "    self.critic = layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.common(inputs)\n",
        "    return self.actor(x), self.critic(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "nWyxJgjLn68c"
      },
      "outputs": [],
      "source": [
        "num_actions = env.action_space.n  # 2\n",
        "num_hidden_units = 12.8\n",
        "\n",
        "model = ActorCritic(num_actions, num_hidden_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2nde2XDs8Gh"
      },
      "source": [
        "### Сбор обучающих данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "5URrbGlDSAGx"
      },
      "outputs": [],
      "source": [
        "# Wrap Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, truncated = env.step(action)\n",
        "  return (state.astype(np.float32),\n",
        "          np.array(reward, np.int32),\n",
        "          np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action],\n",
        "                           [tf.float32, tf.int32, tf.int32])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "a4qVRV063Cl9"
      },
      "outputs": [],
      "source": [
        "def run_episode(\n",
        "    initial_state: tf.Tensor,\n",
        "    model: tf.keras.Model,\n",
        "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  for t in tf.range(max_steps):\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "\n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "\n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "    # Store critic values\n",
        "    values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "\n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "\n",
        "    # Store reward\n",
        "    rewards = rewards.write(t, reward)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  action_probs = action_probs.stack()\n",
        "  values = values.stack()\n",
        "  rewards = rewards.stack()\n",
        "\n",
        "  return action_probs, values, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "jpEwFyl315dl"
      },
      "outputs": [],
      "source": [
        "def get_expected_return(\n",
        "    rewards: tf.Tensor,\n",
        "    gamma: float,\n",
        "    standardize: bool = True) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = reward + gamma * discounted_sum\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  if standardize:\n",
        "    returns = ((returns - tf.math.reduce_mean(returns)) /\n",
        "               (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "  return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhr50_Czxazw"
      },
      "source": [
        "### Actor-Critic loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "9EXwbEez6n9m"
      },
      "outputs": [],
      "source": [
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,\n",
        "    values: tf.Tensor,\n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSYkQOmRfV75"
      },
      "source": [
        "### Функция шага обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "QoccrkF3IFCg"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor,\n",
        "    model: tf.keras.Model,\n",
        "    optimizer: tf.keras.optimizers.Optimizer,\n",
        "    gamma: float,\n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode)\n",
        "\n",
        "    # Calculate the expected returns\n",
        "    returns = get_expected_return(rewards, gamma)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
        "\n",
        "    # Calculate the loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFvZiDoAflGK"
      },
      "source": [
        "### Цикл обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbmBxnzLiUJx",
        "outputId": "8d60092e-f992-4fec-f9c5-e5b5e059f020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:42<00:00,  4.68it/s, episode_reward=-50, running_reward=-50]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Solved at episode 199: average reward: -50.00!\n",
            "CPU times: user 40.5 s, sys: 2.65 s, total: 43.2 s\n",
            "Wall time: 42.8 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "min_episodes_criterion = 100\n",
        "max_episodes = 200\n",
        "max_steps_per_episode = 50\n",
        "\n",
        "# `CartPole-v1` is considered solved if average reward is >= 475 over 500\n",
        "# consecutive trials\n",
        "reward_threshold = 140\n",
        "running_reward = 0\n",
        "\n",
        "# The discount factor for future rewards\n",
        "gamma = 0.99\n",
        "\n",
        "# Keep the last episodes reward\n",
        "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "t = tqdm.trange(max_episodes)\n",
        "for i in t:\n",
        "    initial_state = env.reset()\n",
        "    initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "\n",
        "    episodes_reward.append(episode_reward)\n",
        "    running_reward = statistics.mean(episodes_reward)\n",
        "\n",
        "\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "\n",
        "    # Show the average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "\n",
        "    if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru8BEwS1EmAv"
      },
      "source": [
        "## Визуализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "qbIMMkfmRHyC"
      },
      "outputs": [],
      "source": [
        "# Render an episode and save as a GIF file\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "\n",
        "render_env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
        "\n",
        "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int):\n",
        "  state = env.reset()\n",
        "  state = tf.constant(state, dtype=tf.float32)\n",
        "  screen = env.render()\n",
        "  images = [Image.fromarray(screen[0])]\n",
        "\n",
        "  for i in range(1, max_steps + 1):\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    action_probs, _ = model(state)\n",
        "    action = np.argmax(np.squeeze(action_probs))\n",
        "\n",
        "    state, reward, done, truncated = env.step(action)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render()\n",
        "      images.append(Image.fromarray(screen[0]))\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(render_env, model, max_steps_per_episode)\n",
        "image_file = 'cartpole-v1.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "TLd720SejKmf",
        "outputId": "3a44d0aa-c13f-48ae-b79f-705e4ad21dd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src=\"data:image/gif;base64,R0lGODlhWAKQAYQAAAAAADMAADMzAMzMAAAAMwAzMzMzM8zMMzNmZmZmZplmZv//ZmZmmWaZmZmZmcyZmZnMmczMmczMzP/MzP//zMzM///M/8z//////wAAAAAAAAAAAAAAAAAAAAAAAAAAACH/C05FVFNDQVBFMi4wAwEAAAAsAAAAAFgCkAEACP8AMQgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4ocSbKkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrSo0aNIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNo06pdy7at27dw48qdS7eu3bt48+rdy7ev37+AAwseTLiw4cOIEytezLix48eQI0seOeDAgggUJmvezBnDgM+gLWPuTLq0YdCoUYvObLq167upY6de/bq2bbWyc8emfbu37626g8v+Tby4VOHBeRtfzhwpctWXWTefTp2ocOXVs2v/uTv69u/gfVb/9h6+vPmcAM6rX18zPfv38Fe6j0+/fsj59vPrv4h/v///DfUH4IAEDiRggQj6d2CCDNa3YIMQsvdghBSWN2GFGGp3YYYcNrdhhyAS92GIJNo2YokomnZiiixutmKLMEL2Yow0LjZjjTgadmOOPAa2Y49A8vVjkETeNWSRSMp1ZJJMtrVkk1Ci9WSUVI41ZZVYenVlllxmtWWXYFL1ZZhkPjVmmWgqdWaabBa1ZptwAvVmnHTuNGedeNp0Z558xrRnn4Cy5J4EDkhAKKEOFCoBBtIF6mhNhCZgQAIAVGoAAJJmiqkBl1o6aaIOPCoqShJIeumkCSTgwIqGOpCqpaqG/zrqrBoleqqqi3YUKaUGJErrrxC5CsCkuZpkK6bAJmsQBQxQqipMsiqbrLDPSmutS6V+eu22LDlwaaHchouSt9WKa+5ImRZ77roerZqAuuzGm5G78tZbawAJ2KuvRRJgesG+AEskabQBF7zQqgQbrLBB9C7ssEEJBPDvwxRjgHDFFaeKMcUXb+wwpR47LEGvIS/sLbwl76txygX3mzDL9noLc8Erz6wvBQS8bDO7CRDQ6M7xOhAA0PrWTHS8JB8dLwUAoKx0uA4Q8HTQf04dpqtWr2t01tyWyzW3SX/NbdNih0t22dtWjXaUTK99rQRSuy3tyHJLu2rdypaKd7J07//9a99+zwp44KIOTrijdx9euAGKi4pz449aoDbkOE5OOY2WXw5j5pqzeHbneBrgNOhsek06nKafXrrOqpeJdeuoMw57m4bPTmbttofJee4VSsw7mgg88HuZqQ+P5aTGX51v8l3iznyUuz9fYPTSDyh69VgWjz2Skm5P5eveN+l8+EFSX9by5Ft1vWTgpy+V9o3B735S7U+2/vxNJd6Z+fh71K9p4+ufUIbWmvoJcCgJgMBrtnZAoMhvM2Fr4E8M2Br+SRAiASRNBi84EwvOhYIcpAkAWLdAEobwJQ90TQRPGBMQ9saDLGRU3JjzvxjCBIZ5caENS5LC3vRwhx75YW//7gfEksjsOzjsX9vAs8EiZsRn5dGhEzEiRONUcYoPkWJ1PofFjDSROl/sYoDgo0UxMuSK00GjGQfSPfoQcY0OOaKD4BiRJC6mjHTEwBvpo8Yp9nE7I8xjQvC4HjvOzpCPCeMU97gfQhYReQj6IwcdGR9EXu5xDVIkC6EIIUpyUJLvCaQZPZkfSwZOk/tBpQB9xyFSzg+SHQLl9lz5H1OuTZUDwqX3WFkiWlZPlrU0If58Ob0T6jJBxzQeI1FETN7BkkbAhF0zG2RLoNWQR8mEnQF+hqNpki6aEQqAMJPnzQpVM2XnzE42NbfMHjVAAd5jYJHayTs5QimdD8MneMrp/zd6FgmceANoiUT5O36mSJ/7Quh5DFo2f0JJoF+TJ5cc2jh7lkmh58Loexj6NIjWyKNE42jlRne4dRbpmpfTqH1ECjOQ9silJWMpkHhJOP31yaQ2o2mfZJqxccYJpg/jKZJUCiicZsmmbtOpqIQaMKA+1KchRV+yiBonlPJNdmJTKrCYGi+nZsmr6+JqlqhKJkyKC6lE46S5xLotsIbJrdZiq+6IZtV1GZVdFL2WXGkF1zT1dVR7TRNZk2RWgKG1YmoNWGD79NefQjVmUlXYYLE5WRzdFViVzdFiUfdYezW2TZvlEwC4aa/Lvi2zKEJtkEILps8+yrVdkqjH8mo3Av/uTLUYwm2SWBsl2vaUZ53F2Aq3xVvMmquwTzNtmRI7teK+NLgpgy2Q2thQkj7KuWmTlm7JhN0W+ZZosmUsdIk2XEB111yjXZzitlvIikY2T+KEXLMAJd2mjrdK57WXACqAJ+XarALs1dDp8oug79aNwAOqL8UUnCDqts7AREKwZNEUYFr5N0QV/pVFq8TgkoWXe/eFXIfrI+GNQbhFG05ehkuz4rdhNUktJu57gXRi1X04RiNW2jN5VOKd5Xg7PQZajSOUYvfFWDFHXtoMY5RkeQXZN0NO3o0x9OO1TTlCVXZblm/zZLGVt0IO3mGUSTxjGzYZLxdWMYfO/LC6Moj/zRSTgG2pKcg0N2fMB+wyYwgqSD0nZsug87NhAP3NMvMxxEC8snkI3TpGE8bRjUY0cxQtSEj7hdKCxICl9yLo2WF6OZ223Y73aehMN2TTdAn171Adl0+b+iCsdourXw1rSXdm1rSGdal9E2vs4Xozvd7eryUTbO8VuTZ8zrX/XnyXSP3py8reiJvhIqxKVcrWC+FitO9j3bMQqlPWDndIpr3tj2gbLdUOt7orBZJjl/sj0P7Kt9dNb2t/ZNjvFtius5Luevsb2wQJc76NuO+pZMvfCFc3R4odQ6FphQL9TrjEr5TsgY8LABObyrEmznFrkzYi5Lb4SeL7FIhTquMo/7e2pAUu8pXgmyauAnfKZz4klrd8Je6WE813bm+KxPvmKVlVt3vC86IP3SD9OjrQUVLxoKyq6DQPbs6XzpKXt6RfUF83qlwlqXVDBF9Ux4nQ3ZR1awfgnWhXwDu73nOG9KsBYdfJqH1Sdkul/e4xt/bQU/XxuM/E5jt5etZVRXhXGZ7wKl9Iryzgd5507CdYh/rhJ194VbUdYudufE6sfhKEbz1VMrf2pypP+bwvKGKad3rTcVLvs6v99WsPfelJH6t3HeTxqQcKp3wieNHfHfamrxTtZ0/BuefeJ5QC+EjWPfzKh5v4wy8IpZR+/M3jWSTqhv7krd382Q9kZMyuPv9RRlbwloR79Npne/onL5CBiX8pyRe7yrtfeK4Xav2qegDu368U9+tJ+PgHPqT3epRXAOXHf0KRLdRnEvMXgAQxeb+3dsqHgDfhLdeXEdwXgFIVKcQ3gRRofQr2dBqYMBxIegv4gUcRcx44EZhCf5MnPEiXKIR3giiYFKWyekGnKmoHfQhBLitYg0BBLX1nLAkQgbCHdM5Cg0AIFVHDeRLBgbAHexPwgJxygEt4FQ4gAJjyg25XegoALhaThFeoFtTChQqBKLGyKMJifGN4Fk+HKypxcAzXhi2Rd71ihhazhnNIhzKRKrySKmBIEWgYAMOyh3yIE9/GdlUIiIYHKlxcZyqVEgAI8IUZd4iIUSqgAnp+uImJQgGMZ4mgGIqiOIqkWIqmeIqomIqquIqs2Iqu+IqwGIuyOIu0WIu2eIu4mIu6uIu82Iu++IvAGIzCOIzEWIzGeIzImIy/GBAAIf8LTkVUU0NBUEUyLjADAQAAACznAFQBKwAVAIQAAAAzAAAzMwDMzAAAADMAMzMzMzNmMzPMzDMzZmZmZmaZZmb//2ZmZplmmZmZmZnMmZmZzJnMzJnMzMz/zMz//8zMzP//zP/M//////8AAAAAAAAAAAAAAAAAAAAAAAAI8gAzABhIsKDBgwgHGlCg4AFDAwYFJpxIMYCDBRczLlAAcWCGBxRDGjSgsaTDjhkqiFwJoKFLhzBdepTIcmLMmy8bAsjwceJClxwRLsRJlCPPCQgtYlx6MWjBokQd8qRJkCRTpicL5oTqcurBrWCfgoU6FSlBrjcJjl07lebQtTE7ot3a9izclw4ZPpgLE0JbkC35duUJEyPXAm0lCob5V0HJqw4OTEg8cHHDthNuQvabuOddnIkzQ17wAENnr1tHZ5zcObPDi6xPt4V7VfaDAgo4yz598/HF1gpaUthNPPPYi6anPjCwkLjzo1BZO2wZ+3lAACH/C05FVFNDQVBFMi4wAwEAAAAs5wBCATMAKACEAAAAMwAAMzMAzMwAAAAzMzMzzMwzM2ZmZmZmmWZm//9mZmaZZpmZmZmZzJmZmcyZzMyZzMzM/8zM///MzMz//8z/zP//////AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACP8ALwgcSLCgwYMIDTZAACChw4cQBy4EQBFAg4gYMU6syDGjx4MRGHIc2fDjx5AkU1o0GRGlypcsE0Zo8LImxZgFZxawyTMCzgkNdvIcWvJjUKJIi0bcmBSpz4dMmya9iDCqVKkIr2rlOOHg1q9UC9L8qrIAAgQLEQitmJXsyAAMEsSdm0AtW69uKxagy3eh0LAEx+ZFS3ihYcJ3C07Ia7Gw48Nj25I1C/lx4YSCU5olbJdk5c9oF8pUCVeu6bidK1oGLTqhZr6n/XJkvdphytWOOeIG7TDCSNqQKwK3bFvvbscFGkSYedxw8uIUhxtufeGx6coQBTd3HBitXN4RhW9gN0yQOey5EjCKl442Z4PT1y1kHDueMEjITz0CCF0YPt/8A0VAGU4E7QafWAU8R6BYhZ2XQHcKLujeanHJd0FQ7UmIEHOfLccQAgBquGED0+lkVlciejQRiCmqyJBZIgYEACH/C05FVFNDQVBFMi4wAwEAAAAs7QA7ATkALQCEAAAAMwAAMzMAzMwAAAAzMzMzzMwzM2ZmZmZmmWZmZplm//9mZmaZmWaZZpmZmZmZzJmZmcyZzMyZzMzM/8zMzP/M///MzMz//8z/zP//////AAAAAAAAAAAAAAAAAAAACP8ANQgcSLCgwYMIE2pAAEChw4cQDVpgCADAg4gYMxKcQLFiRY0gH3L0SNJiyJMFH3Qs6RElygcsYwKY4FIjTJkya0a8iVOmBZ0KV/bMCdSg0KFEi2oYibTpRaBMm0rVGVWq1KcheVrd2hCkVq5baWL8CrZkAQQIVCIoUHJnWZwBHCSQSzfBWo8/Hb7FWaCuX5VsP+rdGzOtYZWIDbd0aIFwycSQD6fFOtgxgLORMyMQK9KyRcmaEWP0DLp04rFDzxq++9i0aQ2UFcL1O9du4IquQyNwy7Jv7b+sP+uOnLHw8I4Fjp8u3lq5SdC1iWts7BFzbsOcQdNdDpKk8sMDJyCNpi03w8kJHq9LF2hBZQMF7+NXcOnxu+iC4hE/4PwSt/3dBWVwllICVfRfbAgEEBtQMCWnXlqcTVBAAQQWBMBhc0EWHYALJVchQiqRtxxMC34YHmK//UYBTByaiNCKCND2wAMFHACBixDlN9kDBxTAH447TXgjkBkBViKRCdGYFpIRjbQZkxDReJZ5JgYEACH/C05FVFNDQVBFMi4wAwEAAAAs+AA0ATYANACEAAAAMwAAADMAMzMAzMwAAAAzMzMzzMwzM2ZmZmZmmWZmZplmmZlm//9mZmaZmWaZZpmZmZmZzJmZmcyZzMyZmZnMzMzMzP/M///MzMz//8z/zP//////AAAAAAAAAAAACP8AOQgcSLCgwYMID0YAkLChw4cIFwKYCLGixYIWJmpkeLHjwwQbN3ocaVBiSI0kU5o8KTLlxYwsWbq0CDJmzJkOV9o8iTPizp8RehK0UPPnTqECixr92VPn0qMunT5lOhLm1KsjlV7daCBBggheDbSsKHVrAAgK0KpVkEAsx4dltxpYSxesW4dWt7L8yhesX74UE2rVu/Gv4b5fGxK22fWw4wQWEsbV+7iy4sUnEVfm23Dy1c2OHS7tyrdtZtCIc+5srNm0RtSGHzJu/dctAAOwv0ZWvRe1Rq+G0yo4XJFl7oW7Nav9azHvbdqtd3Ow4Jeu8A0XQx4HSxAD2AcLwIuOv9AxL/THBqkzcMAggvSOhc9rPlggAU4Mv4/bJ1hBgAKhr8kX20BeYYAUBxPh1pdfjkmH24EC5SfgfiC9dyAAQcGWVAAQIkTdedRh2GFCHzbo1YgPUccgUSKi+JIBBrjYUVtByUhWizbCBcB+OebUloU9pvdjkJ3BWCORCtGIZESNLSnkjkAuGQGMPAYZEAAh/wtORVRTQ0FQRTIuMAMBAAAALAEBMwEvACoAhAAAADMAADMzAMzMAAAAMzMzM8zMMzNmZmZmZplmZv//ZmZmmWaZmZmZmcyZmZnMmczMmczMzP//zMzM///M/8z//////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAj/AC0IHEiwoMGDBxsAQMiwocOCCgFIfEixooUIEjMCkGCx40EEGjV6HGkhYsiMEUhWNHlSpMqGGFu2bPASIUiZMmtCxMmTpk6WPHO+jHAzaFCVRY0G5dgRqNKjK59KlZjSYcypUx0mxZqxAAIEDb4W0OjToFOuXsOqBYtg7MSdXE+mZbs2rNuyV+OGpMt3bUaBW/VKrNuXr0DBLecSJlz1rN7FhRuEHYh4L2TIA/MivhwZAWWlXtm2tdyZcAGCQRXzdQuA82K4iUuPBlCgrtrFZT+3lB02I2+wCxLO/K0wpWuwDOUSR1A1guuqByWEPH7bs0DnkaEjJE3dembJYBtoS29IdnlugQ1OezRp3rvAAO47ZuyuNjMABkMl1qZbvW7Vr0y95Ft3AnmlE2U0UWcBSOMdeNFzbTmIEHa4tSYhTOBJRlR8F2pl4UsBAQA7\"/>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}